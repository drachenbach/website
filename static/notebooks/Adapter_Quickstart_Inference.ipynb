{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Adapter-Quickstart-Inference.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PyaAqxoaBgT",
        "colab_type": "text"
      },
      "source": [
        "# **AdapterHub** quickstart example for **inference**\n",
        "\n",
        "First, install adapter-transformers from github/master, import the required modules and load a standard Bert model and tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQwP-DPOvJQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install git+https://github.com/adapter-hub/adapter-transformers.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co43JhjxZ7lT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdapterType\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaCwIyxKC49k",
        "colab_type": "text"
      },
      "source": [
        "Loading existing adapters from our repository is as simple as adding one additional line of code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqNGQxN_C5L8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_adapter(\"sentiment/sst-2@ukp\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNOYYo3MDfOK",
        "colab_type": "text"
      },
      "source": [
        "The [SST adapter](https://adapterhub.ml/adapters/ukp/bert-base-uncased-sst_pfeiffer/) is light-weight: it is only 3MB! At the same time, it achieves results that are [on-par with fully fine-tuned BERT](https://arxiv.org/abs/2007.07779). We can now leverage SST adapter to predict the sentiment of sentences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXHpbKVwwiuz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(sentence):\n",
        "  token_ids = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence))\n",
        "  input_tensor = torch.tensor([token_ids])\n",
        "\n",
        "  # predict output tensor\n",
        "  outputs = model(input_tensor, adapter_names=['sst-2'])\n",
        "\n",
        "  # retrieve the predicted class label\n",
        "  return 'positive' if 1 == torch.argmax(outputs[0]).item() else 'negative'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4v70QiODz8VT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict(\"Those who find ugly meanings in beautiful things are corrupt without being charming.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HieI-Svs0BPP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict(\"There are slow and repetitive parts, but it has just enough spice to keep it interesting.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}