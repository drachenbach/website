{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Adapter-Quickstart-Training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PyaAqxoaBgT",
        "colab_type": "text"
      },
      "source": [
        "# **AdapterHub** quickstart example for **training**\n",
        "\n",
        "This is an adaptation of the HuggingFace [sentence classification notebook](https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/trainer/01_text_classification.ipynb).\n",
        "\n",
        "First, install adapter-transformers from github/master, download the SST dataset, and import the required modules:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQwP-DPOvJQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install git+https://github.com/adapter-hub/adapter-transformers.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRmudIhk8OUv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers\n",
        "!python transformers/utils/download_glue_data.py --tasks SST"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lbwb3NRf8mBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import dataclasses\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, Optional\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, EvalPrediction, GlueDataset, GlueDataTrainingArguments, AutoModelWithHeads, AdapterType\n",
        "from transformers import GlueDataTrainingArguments as DataTrainingArguments\n",
        "from transformers import (\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    glue_compute_metrics,\n",
        "    glue_tasks_num_labels,\n",
        "    set_seed,\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ddUrpFOHItG",
        "colab_type": "text"
      },
      "source": [
        "Training a new task adapter requires only few modifications compared to fully fine-tuning a model with Hugging Face's Trainer. We first configure the training and data arguments (which we would usually set via the command line):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co43JhjxZ7lT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name = \"roberta-base\"\n",
        "\n",
        "data_args = GlueDataTrainingArguments(task_name=\"sst-2\", data_dir=\"./glue_data/SST-2\")\n",
        "training_args = TrainingArguments(\n",
        "    logging_steps=1000, \n",
        "    per_device_train_batch_size=32, \n",
        "    per_device_eval_batch_size=64, \n",
        "    save_steps=1000,\n",
        "    evaluate_during_training=True,\n",
        "    output_dir=\"./models/sst-2\",\n",
        "    overwrite_output_dir=True,\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    do_predict=True,\n",
        "    learning_rate=0.0001,\n",
        "    num_train_epochs=3,\n",
        ")\n",
        "set_seed(training_args.seed)\n",
        "num_labels = glue_tasks_num_labels[data_args.task_name]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyhBjgKbHgtz",
        "colab_type": "text"
      },
      "source": [
        "We then load a pre-trained model (*roberta-base*) and add a new *sst-2* task adapter:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ugxk0RUP8gXx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelWithHeads.from_pretrained(model_name)\n",
        "model.add_adapter(\"sst-2\", AdapterType.text_task)\n",
        "model.train_adapter([\"sst-2\"])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6vjtq3NHtxS",
        "colab_type": "text"
      },
      "source": [
        "By calling `train_adapter([\"sst-2\"])` we freeze all transformer parameters except for the parameters of sst-2 adapter. Before training we add a new classification head to our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVqa_5dLHwGX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add_classification_head(\"sst-2\", num_labels=num_labels)\n",
        "model.set_active_adapters([[\"sst-2\"]])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG0cq4fCH6J3",
        "colab_type": "text"
      },
      "source": [
        "The weights of this classification head can be stored together with the adapter weights to allow for a full reproducibility. The method call model.set_active_adapters([[\"sst-2\"]]) registers the sst-2 adapter as a default for training. This also supports adapter stacking and adapter fusion!\n",
        "\n",
        "We can then train our adapter using the Hugging Face Trainer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXHpbKVwwiuz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = GlueDataset(data_args, tokenizer=tokenizer)\n",
        "eval_dataset = GlueDataset(data_args, tokenizer=tokenizer, mode=\"dev\")\n",
        "\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    return glue_compute_metrics(data_args.task_name, preds, p.label_ids)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUrMl0-XIQab",
        "colab_type": "text"
      },
      "source": [
        "That's it! `model.save_all_adapters('output-path')` exports all adapters. Consider sharing your adapters on AdapterHub!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HieI-Svs0BPP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_all_adapters(\".\")\n",
        "!ls -l sst-2"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}